\chapter{Particle Reconstruction}\label{chap:partreco}
In order to do any sort of analysis, we must be able to compare the data from the experiment to the theory.  The data collected by the ATLAS detector are not particularly easy to interpret at first glance.  The data begin as energy deposits and tracks while the theory consists of simulated hadronized quarks, leptons and neutrinos (this is referred to as truth level MC).  Truth level MC information is largely unused in this analysis.  Instead, the two sets, data and MC, are processed to reconstruct the event and, in the case of MC, include detector information like extra material or overlapping tracks affecting particle reconstruction.  Event reconstruction is applied to form particles and reconstruct the event, so at this level there are quantities such as \met~(missing transverse energy) rather than neutrinos and jets rather than quarks.  We will call this stage of processing (the final stage before analysis) the reconstruction or detector level.  At this point, the two sets, data and MC, should be equivalent (for instance, \met~distributions should be the same between the two if the data are perfectly modeled by the MC). In the following sections, we will give the definitions for a muon, electron, neutrino, or jet at the reconstruction level. We will also include criteria that require particles to be separated and well reconstructed.

\section{Electrons}
Electrons appear to be narrow curving cones of energy in the detector.  The narrow curving track, with a shorter trail of energy depositions through the detector than a muon, is its primary distinguishing feature.  There are electrons that can occur in the detector from sources other than being directly produced in the main collision however, including electrons inside of jets and electrons from photon interactions.  It is also possible to mis-identify narrow jets as electrons, or photons as electrons (such as from bremsstrahlung radiation, produced as the electron interacts in the detector material).  In this analysis, we apply several criteria when identifying if a certain energy deposit and associated tracks are really an electron from the primary collision.

In the ATLAS experiment, there are three different initial electron selections which can be used in different analyses.  These are referred to as loose, medium, and tight, where medium includes loose as well as extra medium requirements, and tight includes both medium and loose as well as extra tight requirements~\cite{Electron}.  The more selections that are applied, the more confidence one has that the particle identified as an electron is really an electron, although you will also remove some real electrons which happen to fail these requirements (making it less efficient).  For this analysis, we prefer to be sure that the particle is what we have identified it as (high purity), so we require the tight selection. Overall, this selection includes requirements to ensure that the energy deposits are narrow and where we expect them to be for an electron (the EM calorimeter), to reduce jets in particular being mis-identified as electrons, and that a track is well matched to this deposit and inner detector deposits, to reduce photon conversions being mis-identified as electrons. 

The requirements for tight electrons are given elsewhere~\cite{Electron}, but we repeat them here for completeness. The loose selection requires the electron $|\eta|<2.47$, low leakage of energy depositions into the hadronic calorimeter, and includes a requirement on energy deposits in the middle of the EM calorimeter, where most electron energy deposits would be expected to be.  The shower width is examined in this layer as well.  The medium selection has additional criteria related to the shower width using the first EM calorimeter layer and the deviation in the energies of the largest and second largest deposits in this layer.  There are requirements related to the track, that there is at least 1 hit in the pixel portion of the inner detector, at least 7 hits from both the pixel and SCT, and that the track's transverse impact parameter, $|d_0|$, is less than 5 mm.  The final medium requriement is related to track and cluster matching, requiring the distance in $|\eta|$ between the cluster in the initial EM layer and the determined track to be less than 0.01.  The final set of selections to make the electron tight include additional cluster and track matching requirements: that the distance in $|\phi|$ between the cluster in the middle EM layer and the determined track be less than 0.02, a requirement on the cluster energy divided by the track momentum, and tightens the $|\eta|$ distance requirement applied for medium electrons from 0.01 to 0.005.  The $|d_0|$ requirement is also tightened to be less than 1 mm.  The TRT portion of the inner detector is used, introducing requirements on the total number of TRT hits and considering the ratio of high threshold hits to total hits in the TRT.  Finally, there are requirements to reduce photon conversions.  The number of B-layer hits (the first pixel detector layer) must be at least one and electron candidates that are matched to reconstructed photon conversions are rejected.

We further require electrons to have a transverse momentum (\pt) of at least 20 GeV.  Electrons must also be isolated, meaning they are not near other particles.  The isolation requirement is specifically optimized for single-top analyses and requires $\rm etcone30/Et < 0.15$ and $\rm ptcone30/Et < 0.10$.  Etcone30 and ptcone30 refer to the amount of transverse energy deposited or track momentum in a cone around the electron track(s) with a $R$ of 0.3, where $\Delta R = \sqrt{\Delta \eta^2 + \Delta \phi^2} < 0.3$.  Electrons must also have $|\eta| < 2.47$, and exclude the region $1.37 < |\eta| < 1.52$ due to detector limitations.  Additionally, if electrons fall within the LAr hole, then they are not considered to be electrons.

\section{Muons}
Muons are primarily distinguished by their relatively long lifetimes and long, curved tracks which reach into the muon calorimeter section of the detector.  Muons are required to satisfy several strict quality requirements.  As with electrons, the muons have several categories for an initial identification definition.  In this case, the categories refer to different muon reconstruction algorithms.  The one used here is the ``combined muon''~\cite{Muon1, Muon2Wjet} algorithm, which considers both inner detector and muon spectrometer tracks, which are reconstructed separately.  A combined fit is performed on the tracks from the two detectors to form a final muon track.  If a combined track cannot be formed, the particle is not considered to be a muon.  Of the different algorithms, this is the one that has the highest purity.

%https://twiki.cern.ch/twiki/bin/view/AtlasProtected/BTaggingJetTagNtuple
%xpectbla[ntracks]:true if track crossed an active pixel module on b-layer 

%https://twiki.cern.ch/twiki/bin/view/AtlasPublic/InDetTrackingPerformanceApprovedPlots
%A hole is defined as a missing measurement when it is expected i.e. a track crosses an operational silicon modules and have hits both in silicon modules proceeding and following it. Since tracking runs with a requirement of no holes in the Pixel Detector, only SCT holes can be counted. 

There are a few track quality requirements used to define a muon which are related to inner detector information, including at least one B-layer hit, at least two pixel hits, and SCT and TRT hit and quality requirements.  These are a bit detailed, and are given here for completeness.  We require the flag expectBLayerHit to be false or the number of BLayer hits $> 0$, meaning there must be a hit in the B-layer unless the track passes through a dead area of the detector.  A muon must have the number of pixel hits plus the number of crossed dead pixel sensors $\ge 2$, the number of SCT hits plus the number of crossed dead SCT sensors $\ge 6$, and the number of pixel holes plus the number of SCT holes $\le 2$.  Holes are where a module did not respond as expected, even though modules elsewhere along the track did.  Finally, there is a complex requirement on the number of TRT hits divided by the number of outliers related to the quality of the track fit, where outliers are hits that deviate from the track.  We require, where $n$ is the number of TRT hits plus the number of TRT outliers, $n\ge 6$ and the number of TRT outliers divided by $n$ to be $<0.9$ for $|\eta|<1.9$.  Then we also require the number of TRT outliers divided by $n$ to be $<0.9$ if $n\ge 6$ for $|\eta|\ge 1.9$.  In this last case, if $n< 6$, the event will pass, unlike the first case.

The isolation requirement is the same as the electron isolation requirement, namely that $\rm etcone30/Et < 0.15$ and $ptcone30/Et < 0.10$.  The muons we select are specifically not allowed to overlap in position with jets, meaning any muon candidate within $\Delta R$ of 0.4 of a jet is not considered.  For this purpose, we consider all jets with \pt~above 20 GeV and include jets that overlap with electrons.  Additionally muons must have $\rm \pt > 20$ GeV and $|\eta| < 2.5$.

\section{Quarks}\label{sec:quarks}
%not sure I understand collinear safety when two jets overlap.  is it because they are so parallel there isn't a boundry split issue?  soft particles combined with hard particles before these are combined with each other to make some jet???  minimum jet distance so jets are well separated???
Perhaps the most complex reconstructed objects in the detector are jets.  Jets are hadronized quarks, showers of many particles that tend to be absorbed in the hadronic calorimeter.  Because they are basically sprays of particles, it is possible for them to overlap and be in odd shapes.  In order to work with these, we need to understand which energy deposits correspond to which jets.

The method used to form jets in this analysis is an algorithm called the anti-$k_{t}$ algorithm~\cite{AntiKt}.  There are two major jet algorithm types, cone and clustering algorithms, where anti-$k_{t}$ is a clustering algorithm that forms jets that happen to have very cone-like shapes.  A clustering algorithm is a bottom-up algorithm that combines individual tracks together to form a jet, while a cone algorithm is a top-down algorithm which forms a cone for the jet and considers deposits within that cone part of the jet.  For this algorithm, the area in $\eta - \phi$ space is $\pi R^{2}$.  This is the area containing energy deposits associated to one particular jet, assuming that there are no other high $p_{T}$ (hard) objects within a distance $2R$.  For this analysis we use a cone size $R$ of 0.4.  If there is another hard jet, the harder jet will have a cone shape and the lower $p_{T}$ (soft) jet will have a crescent shape.  The anti-$k_{t}$ algorithm is used because not only is it reasonably fast, but the jets are grouped using the highest $p_{T}$ energy deposits first and then looking at surrounding objects, meaning that random low energy deposits will not change the jet shape.  This means it is infrared safe because it avoids potential divergences from an infinite number of very soft, low energy jets.  Also, each deposit and track is assigned to some jet.  There is no splitting and merging of overlapping jets, so it doesn't matter if a jet is split into two parallel (collinear) particles, making it collinear safe.

The jet energies are corrected from what is actually measured in the detector to what we have in the MC simulation.  The first correction is to the ``EM scale'', the main correction to the detected energy, based on test beam and cosmic ray studies.  The second correction is a jet energy scale (JES) correction~\cite{JESnew,JES}, an additional calibration based on the jet \pt~and $\eta$. It includes corrections for losses due to dead material or leakage energy from particles depositing energy outside the hadronic calorimeter.  The jets are thus called ``EM-JES'' calibrated jets~\cite{JER}.  Specifically, we use jets called AntiKt4TopoEMJets, where Topo refers to topological clusters~\cite{JESnew, Topo}.  This is an algorithm that clusters energy deposits together for the jet, by starting with an energetic deposit with a signal to noise ratio greater than 4, and adding neighboring deposits that have a signal to noise ratio greater than 2.

We have additional quality requirements.  We remove any jets that have been reconstructed with corrected energy that is negative and thus are not physical (this is a very small effect).  Any jet candidates that overlap with electrons with $\Delta R < 0.2$ are not considered.  Further, we require jets to have $\pt > 25$ GeV and $|\eta| < 4.5$.  Notice that this is a much more forward requirement than that of the leptons.  The calorimeters allow information this far forward in the detector and it is particularly important information for our analysis.

\subsection{b-tagging}
 \label{sec:Btag}
There are two subsets of jets that are used frequently in this analysis, tagged and untagged.  Jets that are b-tagged (tagged) are required to have a high probability of being a hadronized b-quark.  The rest remaining jets are referred to as untagged.  Additionally, jets must have $|\eta| < 2.5$ to be b-tagged because of the inner detector range.  This means all jets with $|\eta| > 2.5$ are considered untagged.

\subsubsection{How b-quarks are Identified}\label{sec:Bdist}
%compare distance to c or u quarks?
The bottom quark is an unusual particle.  It is heavy, as mentioned above, and it also travels quite a long distance, relatively, from its creation before it forms a jet (b-jet).  The lifetime of the b-quark is about $\tau = 1 \times 10^{-12}$ seconds.  We can determine the distance it should travel, on average, by assuming an energy of about 40 GeV.  Because $E=\gamma mc^{2}$ and $d = \gamma c \tau$, where $\tau$ is the lifetime, m is the mass (about 4 GeV/c$^2$) and E is the energy, we can write, where c is the speed of light $c=3\times 10^3~m/s$):
\begin{equation} d = \frac{E}{mc^{2}} c \tau \approx \frac{40}{4}\cdot 3 \times 10^{8}m/s\cdot 1 \times 10^{-12}s = 0.003~m \end{equation}
This means that the b's travel about 3 mm from the main interaction point before forming a jet.

Tracks from the inner detector can be reconstructed and traced back inside the beam pipe (where there is, of course, no detector).  These tracks will then intersect within the beam pipe, which has a diameter of about 6 cm.  Most intersect in a primary vertex, the place the proton-proton collision occurred.  However, some may intersect in other places, secondary vertices, where a b hadron has formed a jet (see Figure~\ref{fig:Display_bjet}).  Of course, as there are multiple proton-proton collisions producing pileup events, and just more particles in general, it can be difficult to really distinguish which tracks go where, and to which vertex.  This is why the inner detector resolution is so very important and also the reconstruction algorithms to determine these vertices.  Because of the importance of the inner detector, b-tagged jets are only defined within its range, $|\eta|<2.5$.

\begin{figure}[!h!tpb]
 \centering
 \includegraphics[width=1.00\textwidth]{figures/detector/bjet.eps}
\vspace{-0.5cm}
 \caption{Event display for a b-jet, with the secondary vertex shown in the dashed box and primary vertex shown as a round ball~\cite{EventDisplayBTag}, ATLAS Experiment \copyright 2011 CERN}
 \label{fig:Display_bjet}
 \end{figure}

The chosen b-tagger is just a distribution related to the likelihood of a jet coming from a b, and a jet is b-tagged based on whether the b-tagger value for that jet is above or below a certain threshold, called an operating point.  A jet is considered to be mis-tagged if the jet was not really from a b-hadron but was still b-tagged.  Different operating points have different levels of performance.  There are two major ways to determine performance, the b-tagging efficiency and the mis-tagging efficiency.  These two measures are inversely proportional, so a high b-tagging efficiency sample will have low mis-tagging efficiency.  This means that if the b-tagging efficiency is high, most of the jets that are really b-hadrons will be b-tagged, but there will also be a relatively high proportion of jets that were not really b-hadrons that were nevertheless b-tagged as well.

In our case, our final state has both a b-jet and a (typically) light quark jet.  Our large backgrounds before b-tagging are backgrounds with many light jets in them.  Therefore, while we remove some of our signal by having a lower b-tagging efficiency, we prefer to remove proportionally more of our background by choosing an operating point with a high mis-tagging efficiency.  Even though fewer jets are b-tagged, we have more confidence that the ones we do b-tag are really b-hadrons than if we had chosen a higher b-tagging efficiency.

The b-tagger used in this analysis is the JetFitterCombNN b-tagger~\cite{BTag:CONF}.  This is a combination or two b-taggers called JetFitter and IP3D.  The JetFitter algorithm uses a Kalman filter~\cite{Kalman} to determine the path along which b and c hadrons (from decays inside the b-jet) and the primary vertex lie, and this determines a track for the b-jet.  Additional discrimination based on the secondary vertex and its uncertainty is done using a likelihood method.  The IP3D b-tagger uses the impact parameter information in all three dimensions with a likelihood technique to discriminate between the b-jets and lighter jets.  An impact parameter is the shortest distance from the primary vertex (interaction point) to a track.  The transverse impact parameter is a common quantity known as $d_{0}$, and IP3D uses both transverse and longitudinal ($z_{0}$) impact parameter information.  The JetFitterCombNN actually uses both JetFitter and IP3D and forms a neural network based using information from these two algorithms.  The output of the neural network forms the JetFitterCombNN b-tagger.  For more information, please see~\cite{BTag:CONF}.


\subsubsection{Impact of Different Operation Points on the Analysis}
For this analysis, we choose the JetFitterCombNN b-tagger with a threshold of 2.4, so if the JetFitterCombNN value is $> 2.4$ the jet is b-tagged.  This gives a 57\% b-tagging efficiency, but a very high light quark rejection of about 1000~\cite{BTag:CONF} (or a mis-tagging rate of about 0.1\%).  This is the lowest b-tagging efficiency (and highest mis-tagging efficiency) operating point approved for use.  In Figure~\ref{fig:OPThreshold_bjet}, the effect on the yields of using different b-tagging operating points can be seen, where the chosen operating point is shown with a vertical line.  The yields for each process are given for a particular threshold for the JetFitterCombNN b-tagging variable, as well as scaled versions of the signal divided by the background or the square root of the background.  Both of these are rough indications of signal separation and analysis performance (but note that they don't include systematic uncertainties).  In general it is clear that while the $t$-channel yields go up for a looser operating point, the backgrounds also increase, at a greater rate.  The separation appears to be better for higher thresholds.  Although we lose some of our overall event yield, the background is reduced at a greater rate than the signal is reduced.  For this analysis, we use the highest threshold available, 2.4.

%discuss b-tagging scale factors here?  data based determination, aka pt rel?
\begin{figure}[!h!tpb]
 \centering
 \includegraphics[width=1.0\textwidth]{figures/btag/2j1b_btaglog_nostack.eps}
 \includegraphics[width=1.0\textwidth]{figures/btag/3j1b_btaglog_nostack.eps}
\vspace{-0.5cm}
 \caption{Distribution of the yields for the signal (t-channel) and its backgrounds, given a selection on the JetFitterCombNN b-tagging variable at the given x-axis value.  Selections at higher x-axis values have lower b-tagging efficiencies but higher mis-tagging efficiencies.  The black vertical line shows the threshold used in the analysis.}
 \label{fig:OPThreshold_bjet}
 \end{figure}

\section{Taus}
Thus far, tau leptons have not really been discussed.  This is because we do not specifically reconstruct or select for taus in this analysis, although of course this is a lepton that could be involved in the W decay from the top quark.  The reason for this is the short tau decay time and the nature of its decay particles.  Unlike the other two leptons, electrons and muons, taus do not travel very far into the detector before decaying (it has a lifetime of about $3\times 10^{-13} s$ and a mass of about 1.8 GeV~\cite{PDGSummary}).  Taus may decay into quarks, at which point they look like a jet (it is theoretically possible to reconstruct a tau like a b-jet, but that is not currently done).  It is also possible for the tau to decay to the one of the other two lepton types, plus neutrinos (this happens about 40\% of the time).  In this case, we incidentally select for these when we select for a muon or electron in our event.  However, we don't specifically select or reconstruct a tau.  The MC and data both have taus in them and the same selection (and lack of special reconstruction) is applied in both cases, so this is consistent.

\section{Neutrinos and Missing Energy}\label{sec:Neutrinos} 
%mention lepton z from E and m?
Although there is a neutrino in the t-channel single-top final state, it has not been heavily discussed.  This is because neutrinos interact weakly (and not very often).  Thus, we do not try to detect the neutrinos but instead use energy conservation to determine the missing transverse energy, or \met, which corresponds to the neutrino's \pt~(or sum of the neutrino \pt~values).  If there is more than one neutrino, of course, there is still just one \met~value.  Information about the momentum in the z direction is not preserved as there is no information collected inside of the beam pipe.  We can't determine how much energy is missing in that direction from particles that may have traveled only in the z direction and missed the detector.  We do have information everywhere else though, so we are able to determine the missing energy in the x and y directions.

The \met for ATLAS~\cite{MET} is calculated from the sum of the calorimeter energy deposits (includes jets, electrons, photons, and $|tau$ decays as well as other deposits) and the energy from reconstructed muons in the x and y directions.  The portion of the calorimeter energy deposits in the ``other'' catagory are called cell out deposits and jets with \pt below 20 GeV but above 7 GeV (too low to be considered jets in this analysis) have energy deposits classified as soft jet deposits, rather than standard jet deposits.  Cell out deposits are often low energy deposits too low to be classified as soft jets or other physics objects.  When all of these contributions are summed, because the inital collision is in the z direction with no energy in the x or y directions, we expect the sum to be 0.  The deviation from 0 gives the \met.

For this analysis, we will want to use the momentum information in the longitudinal or z direcation, so it is reconstructed from information about the W boson.  The single lepton and \met~(which we expect comes from one neutrino in our selected events) correspond to a single W boson.  The mass of the W is well known, and we take it to be 80.42 GeV, consistent with the current Particle Data Group value~\cite{PDGSummary}.  From this information, we can reconstruct the neutrino momentum in the z direction, using the following equation and taking the neutrino and lepton masses to be zero:
\begin{equation} P^{\mu}P_{\mu} = M^{2}_{W} \end{equation}
Here, P is the four-momentum of the lepton and neutrino combination, and $M_{W}$ is the W mass.

After some manipulation, the equation can be rewritten as:
\begin{equation} l_e^2\nu_{pT}^2 - (A+B)^2 + (l_e^2-l_{pz}^2)\nu^2_{pz} + 2(A+B)l_{pz}\nu_{pz} = 0\end{equation}
where $A= \frac{M^{2}_{W}}{2}$ and $B=l_{px}\nu_{px} + l_{py}\nu_{py}$.  Here, $\nu$ is the neutrino and $l$ is the lepton where $l_e$ is the lepton energy, $l_{pz}$ is the lepton longitudinal momentum and $l_{pT}^2 = l_{px}^2 + l_{py}^2$ is the lepton transverse momentum.  The quantity $\nu_{pT}$ is just \met.  This is a quadratic equation which can be solved in the usual way for $\nu_{pz}$.  In the solution, it is possible to have two results, and in this case we take the smaller of the two values.  It is also possible to have a negative discriminant ($1+(\nu_{pT}^2 l_{pz}^2) < \nu_{pT}^2 l_e^2$) and in that case a 0 value is taken.  The \met~is primarily used in the analysis until the final cut-based selections are applied, but the reconstructed neutrino $p_z$ is used for quantities such as the reconstructed top mass.
%1.0E0 + pow(nu_pt,2) * (pow(l_pz,2)-pow(l_e,2))