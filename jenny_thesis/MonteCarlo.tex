\chapter{Monte Carlo Simulation and Corrections}\label{chap:MC}
The Monte Carlo (MC) which simulates events produced by the proton-proton collisions does not simply appear from equations produced by high energy physics theorists, but instead must be produced and corrected using computer algorithms.  There are two steps to this process, a generator that produces the general event with quarks, leptons and neutrinos, and then a showering algorithm that adds extra jets and gluons to these ``simple'' processes made by the generator so they resemble more closely what we see in the detector, where higher-ordered processes are produced.  Additionally, after considering the effects of the detector and producing what is called reconstruction level MC from the truth level MC (which does not include detector effects), there are still corrections to be made to the particle energies and the MC events in general to better match the data.  In this chapter we discuss the MC production and these additional MC corrections.

\section{MC Generator and Showering}
%discuss matching scheme between shower/generator, scales, pt cut offs?  single-top special??
%MC settings: https://twiki.cern.ch/twiki/bin/view/AtlasProtected/McProductionCommonParameters
%add details about versions?
%AcerMC for single-top http://indico.cern.ch/getFile.py/access?contribId=20&sessionId=1&resId=0&materialId=slides&confId=3757

The simulation of MC requires a few processing steps.  The first is a MC generator which reproduces the basic Feynman diagrams and the second is a showerer, which adds on additional particles.  In practice it isn't quite so clear-cut.  There can be some overlap between what the showerer and the generator do, and diagram overlap removal procedures are applied in this case.  There may also be some uncovered diagrams depending on the process and choice of showering algorithm or generator.

For this analysis we use {\sc Pythia}~\cite{SAMPLES-PYTHIA} for the parton showering for the single-top processes and {\sc Herwig} ~\cite{SAMPLES-HERWIG} with {\sc Jimmy}~\cite{SAMPLES-JIMMY} for the showering for all other processes.  The leading order parton density functions are from {\sc CTEQ6L}~\cite{cteq6}.  The generators are more varied.  For our signal we use {\sc AcerMC}~\cite{SAMPLES-ACER}, which in our studies uses a procedure to reproduce an extra, soft b-quark from a incoming gluon more correctly than alternative generators~\cite{SAMPLES-ACER:tchan}.  The \ttbar~process is generated using {\sc MC@NLO}~\cite{SAMPLES-MCNLO} generator, which includes more diagrams than a standard leading-order generator.  The W+jets and Z+jets processes use {\sc Alpgen}~\cite{SAMPLES-ALPGEN}, as do diboson simulations.

Generators and showering algorithms are updated frequently as they are tuned to better match the data.  There is some uncertainty related to our measurement because we know these probably don't precisely match our data.  However, the extensive studies with Tevatron data have produced generators and showering algorithms that match up well with LHC data, and the agreement will be discussed in Chapter~\ref{chap:variables}.  Additionally, the MC used in this analysis has the ATLAS tag of ``MC10b''.  It has three simulated bunch trains with 225 ns separation between the trains.  Each train has 36 filled bunches with 50 ns separation between bunches, the same separation as the data.  

Finally, it should be noted that the final simulation of the particles going through the material of the ATLAS detector is done with separate programs~\cite{ATLASSIM} based on {\sc GEANT4} software~\cite{GEANT}.  This stage of processing accounts for the specific configuration of the ATLAS detector, including regions where there may be more or less material.  This stage also introduces simulation of the resolution in different portions of the detector.  The events are then reconstructed in the same was as data collected by ATLAS.

\section{Monte Carlo Weighting and Corrections}
When the Monte Carlo (MC) is produced, no particular attention is paid to the number of events generated, other than to be sure there are enough events to allow a sufficient variety of kinematics (and make the statistical uncertainty low).  In order to compare the MC to the data sample, the MC must be weighted so that the proportions of the different processes are as we expect and the overall normalization is correct.  Here we describe how the event weighting is done and what weights and corrections are applied.

%MCatNLO weight
%b-tagging weight
%lepton SF
%discuss lepton energy smearing somewhere  scaling vs smearing
%pileup weight and redo of MC Event count
%W+jets weight- short mention, reference to next section?  Or just discuss in next section

\subsection{Theoretical cross-section and luminosity weight}
The first and perhaps most important weight is the factor that normalizes each process to its theoretical cross-section and also the whole MC sample to the number of events expected for the amount of data we have (the integrated luminosity).  The factor multiplied onto each MC event is formed as:
\begin{equation} \frac{XS \cdot BR \cdot K \cdot L}{N_{MC}} \end{equation}
where $XS \cdot BR \cdot K$ is the cross-section times branching ratio times k-factor discussed in Section~\ref{sec:ExpectedCrosssections}, $L$ is the integrated luminosity (1035.27 $pb^{-1}$) and $N_{MC}$ is the number of Monte Carlo events.  The numerator is simply the number of expected events from Equation~\ref{equ:numberofevents}, with the cross-section written out with corrections.  The values for $XS \cdot BR \cdot K$ are given in Table~\ref{TABLE-MCSAMPLES}.

The denominator is the number of MC events, as stated.  However, it is important that this include any weights that can affect the overall number of event before analysis selections.  These include the pileup weight, discussed next, and negative weights associated with MC generators.  Certain MC generators, particularly NLO top-quark process generators like {\sc MC@NLO} (\ttbar) or {\sc AcerMC} (single-top production) give some events negative weights during generation related to interference effects when including NLO processes.  This weight is applied when determining the $N_{MC}$ value, as well as later in the analysis as another event weight.  

\subsection{Pile-up weight}
The other weight applied to the $N_{MC}$ value (and all MC events) is the pileup weight, which is related to the number of primary vertices.  This is a special weight to adjust the MC to represent the events one expects under certain (data-like) pileup conditions.  The pileup conditions may change after a large sample of MC is generated, so this allows more flexibility.  The difference between the $N_{MC}$ in the full MC sample with and without this pileup weight is typically about 1\%, so the effect on $N_{MC}$ is minimal.

The weight itself ranges in value from about 0 to 5, where many events are given weight 0 because they are simulated with pileup conditions exceeding the current data conditions.  This of course has an effect on the MC statistical uncertainty, as the MC statistics are effectively reduced in this case.

\subsection{Lepton scale factor}
The lepton scale factor is a factor used to adjust the MC so that the lepton efficiencies match those found in the data.  Scale factors are discrete numbers applied to the MC which may have some dependency on \pt~or $\eta$.  There are different scale factors related to the trigger, reconstruction and identification for each lepton type.  These scale factors are all approximately 1~\cite{Electron, Muon1, Muon2} and have a minimal impact on the analysis.

\subsection{Mis-tagging and b-tagging scale factor}
Like the leptons, the b-tagging and mis-tagging efficiencies we see in data are not exactly the same as MC, so we apply a scale factor to correct the MC.  This scale factor is also typically close to 1~\cite{BTag:CONF, BTag:CONF2}.  However, the uncertainties on the b-tagging scale factor are larger than the others for this analysis, so the b-tagging scale factor has an increased level of importance.  It would be possible to eliminate this scale factor and uncertainty if b-tagging were not used in the analysis, but the signal separation is not sufficient to do this with the current data total.  For more details on b-tagging and mis-tagging efficiencies in this analysis, see Section~\ref{sec:Btag}.

\subsection{Energy corrections in the analysis}\label{sec:energyresolution} 
The energies of different particles are not necessarily quite the same in MC as they are in data.  Although some corrections are applied to the files before they reach analysers, there is some fine-tuning done at the analysis level.  We can smear or scale the particle energy, where smearing involves changing the particle energy using some distribution, like a Gaussian.  This may be done as an uncertainty on the analysis, as is the case with jets, or it may be applied to the nominal sample, as is the case for leptons.  

In the case of the leptons, the corrections are chosen to have a better match between the Z mass peak and width in the data and MC.  The electrons have two instances of energy corrections~\cite{Electron}.  The first is a scaling done in the data to correct the energy of the electrons.  This sort of correction is usually done before the analysis level, as for jets, but in this case it is done afterwards.  The second is a smearing, done in MC, to adjust the width of the Z peak to what we see in data.  For the muons, scaling and smearing are both applied to the MC only.  In this case, because tracks are used from the inner detector and muon spectrometer to form the full muon track, there are separate corrections on the tracks in each region~\cite{MuonER}.

For the jets, the jet energy scale (JES) correction was applied when the jets are formed and was discussed in Section~\ref{sec:quarks}.  This is a factor that adjusts the jet energy to account for issues such as dead material or energy leakage.  The jet energy resolution (JER) corrections are done in a separate iteration of the analysis and the difference from the nominal sample is taken as an uncertainty~\cite{JER, JER2}.  The JER adjustment affects the jet energy by adjusting the value according to a Gaussian distribution, where the Gaussian width depends on the jet \pt and $\eta$ value.  There are two major techniques to determine the jet energy resolution, a di-jet balance method and a bi-sector method.  The di-jet balance method uses a two jet event where the jets are both expected to have a similar \pt value (back to back jets).  The deviation between the jet \pt values divided by the sum for each jet can be plotted and fit with a Gaussian distribution.  The bi-sector method is slightly different.  It is based on the sum of the \pt in a 2 jet, back to back event.  This \pt vector is projected into the transverse plane (transverse to the beam axis) such that the $\eta$ coordinate in this plane bisects the difference in the $\phi$ angles of the two jets.  The deviations from nominal (where the sum of the jet \pt values is 0) for each tranverse plane angle is considered separately and can be found by fitting gaussian distributions to the \pt values for each angular component in different \pt ranges.  The function used to adjust the jet energy in this analysis is based on a combination of the results of these two techniques.


